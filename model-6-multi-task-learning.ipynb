{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-29T10:03:35.984746Z","iopub.execute_input":"2023-07-29T10:03:35.985538Z","iopub.status.idle":"2023-07-29T10:03:36.032095Z","shell.execute_reply.started":"2023-07-29T10:03:35.985500Z","shell.execute_reply":"2023-07-29T10:03:36.031083Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/maternal-and-child-health-monitoring-in-lmics/training_label.csv\n/kaggle/input/maternal-and-child-health-monitoring-in-lmics/sample submission.csv\n/kaggle/input/maternal-and-child-health-monitoring-in-lmics/gee_features.csv\n/kaggle/input/selecting-features-using-tree/y_train.csv\n/kaggle/input/selecting-features-using-tree/x_train.csv\n/kaggle/input/selecting-features-using-tree/__results__.html\n/kaggle/input/selecting-features-using-tree/x_submission.csv\n/kaggle/input/selecting-features-using-tree/__notebook__.ipynb\n/kaggle/input/selecting-features-using-tree/DHSID.csv\n/kaggle/input/selecting-features-using-tree/__output__.json\n/kaggle/input/selecting-features-using-tree/custom.css\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loading the required libraries and packages","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport tqdm\nfrom scipy import stats\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:03:38.541227Z","iopub.execute_input":"2023-07-29T10:03:38.541639Z","iopub.status.idle":"2023-07-29T10:03:53.821982Z","shell.execute_reply.started":"2023-07-29T10:03:38.541607Z","shell.execute_reply":"2023-07-29T10:03:53.820867Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:03:53.824320Z","iopub.execute_input":"2023-07-29T10:03:53.825069Z","iopub.status.idle":"2023-07-29T10:03:53.830862Z","shell.execute_reply.started":"2023-07-29T10:03:53.825033Z","shell.execute_reply":"2023-07-29T10:03:53.829611Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Checking whether cuda is available or not","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"DEVICE IS ... \", device)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:03:53.832524Z","iopub.execute_input":"2023-07-29T10:03:53.832929Z","iopub.status.idle":"2023-07-29T10:03:53.851253Z","shell.execute_reply.started":"2023-07-29T10:03:53.832892Z","shell.execute_reply":"2023-07-29T10:03:53.850124Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"DEVICE IS ...  cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs = 5\nlr = 1e-4\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:09:16.926596Z","iopub.execute_input":"2023-07-29T13:09:16.927205Z","iopub.status.idle":"2023-07-29T13:09:16.932310Z","shell.execute_reply.started":"2023-07-29T13:09:16.927168Z","shell.execute_reply":"2023-07-29T13:09:16.931069Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"Loading the datsets","metadata":{}},{"cell_type":"code","source":"x_train = pd.read_csv('/kaggle/input/selecting-features-using-tree/x_train.csv')\ny_train = pd.read_csv('/kaggle/input/selecting-features-using-tree/y_train.csv')\nx_submission = pd.read_csv('/kaggle/input/selecting-features-using-tree/x_submission.csv')\nDHSID = pd.read_csv('/kaggle/input/selecting-features-using-tree/DHSID.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:03:53.866598Z","iopub.execute_input":"2023-07-29T10:03:53.867072Z","iopub.status.idle":"2023-07-29T10:04:51.654147Z","shell.execute_reply.started":"2023-07-29T10:03:53.867025Z","shell.execute_reply":"2023-07-29T10:04:51.652903Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Scaling the data","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Stack the DataFrames vertically\nmerged_df = pd.concat([x_train, x_submission], axis=0)\n\n# Separate the column names from the DataFrame\ncolumn_names = merged_df.columns.tolist()\n\nscaled_data = scaler.fit_transform(merged_df)\n\nscaled_df = pd.DataFrame(scaled_data, columns=column_names)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:51.656219Z","iopub.execute_input":"2023-07-29T10:04:51.657157Z","iopub.status.idle":"2023-07-29T10:04:57.420587Z","shell.execute_reply.started":"2023-07-29T10:04:51.657106Z","shell.execute_reply":"2023-07-29T10:04:57.419058Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Separating the DataFrame back into the original parts\nx_train_scaled = scaled_df[:len(x_train)]\nx_submission_scaled = scaled_df[len(x_train):]","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:57.422003Z","iopub.execute_input":"2023-07-29T10:04:57.422359Z","iopub.status.idle":"2023-07-29T10:04:57.428758Z","shell.execute_reply.started":"2023-07-29T10:04:57.422329Z","shell.execute_reply":"2023-07-29T10:04:57.427402Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Set the random seed for reproducibility\nrandom_seed = 42\nnp.random.seed(random_seed)\n\n# Splitting the data into train and test sets based on country (80% train, 20% test)\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x_train_scaled, y_train, test_size=0.2, stratify = x_train_scaled[\"key1\"])\n\n# Splitting the train set into train and val sets based on country (75% train, 25% dev)\nx_train1, x_val1, y_train1, y_val1 = train_test_split(x_train1, y_train1, test_size=0.25, stratify = x_train1[\"key1\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:57.430132Z","iopub.execute_input":"2023-07-29T10:04:57.430499Z","iopub.status.idle":"2023-07-29T10:04:59.725543Z","shell.execute_reply.started":"2023-07-29T10:04:57.430465Z","shell.execute_reply":"2023-07-29T10:04:59.724333Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"y_train1.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:59.726987Z","iopub.execute_input":"2023-07-29T10:04:59.727920Z","iopub.status.idle":"2023-07-29T10:04:59.736652Z","shell.execute_reply.started":"2023-07-29T10:04:59.727878Z","shell.execute_reply":"2023-07-29T10:04:59.735133Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(46768, 6)"},"metadata":{}}]},{"cell_type":"code","source":"x_test1.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:59.742764Z","iopub.execute_input":"2023-07-29T10:04:59.743543Z","iopub.status.idle":"2023-07-29T10:04:59.752086Z","shell.execute_reply.started":"2023-07-29T10:04:59.743507Z","shell.execute_reply":"2023-07-29T10:04:59.750496Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(15590, 2000)"},"metadata":{}}]},{"cell_type":"code","source":"x_val1.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:59.753524Z","iopub.execute_input":"2023-07-29T10:04:59.753924Z","iopub.status.idle":"2023-07-29T10:04:59.765067Z","shell.execute_reply.started":"2023-07-29T10:04:59.753878Z","shell.execute_reply":"2023-07-29T10:04:59.764209Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(15590, 2000)"},"metadata":{}}]},{"cell_type":"code","source":"y_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:59.766184Z","iopub.execute_input":"2023-07-29T10:04:59.767326Z","iopub.status.idle":"2023-07-29T10:04:59.778503Z","shell.execute_reply.started":"2023-07-29T10:04:59.767289Z","shell.execute_reply":"2023-07-29T10:04:59.777640Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Index(['Mean_BMI', 'Median_BMI', 'Unmet_Need_Rate', 'Under5_Mortality_Rate',\n       'Skilled_Birth_Attendant_Rate', 'Stunted_Rate'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"**Multitask learning model**","metadata":{}},{"cell_type":"code","source":"class MultiTaskLearn(nn.Module):\n    def __init__(self, embed_dim=2000, layers=[2000, 512, 256, 256, 128]):\n        super().__init__()\n\n        self.embedding_dim = embed_dim\n\n        self.mlp_network = nn.Sequential(\n            nn.Linear(layers[0], layers[1]),  \n            nn.ReLU(),\n            nn.Linear(layers[1], layers[2]), \n            nn.ReLU(),\n            nn.Linear(layers[2], layers[3]),\n            nn.ReLU(),\n        )\n\n        self.last_layer_mean_bmi = nn.Linear(\n            layers[3], 1\n        )  \n        self.last_layer_median_bmi = nn.Linear(\n            layers[3], 1\n        )  \n        self.last_layer_Unmet_Need_Rate = nn.Linear(\n            layers[3], 1\n        )  \n        self.last_layer_Under5_Mortality_Rate = nn.Linear(\n            layers[3], 1\n        )  \n        self.last_layer_Skilled_Birth_Attendant_Rate = nn.Linear(\n            layers[3], 1\n        )  \n        self.last_layer_Stunted_Rate = nn.Linear(\n            layers[3], 1\n        ) \n\n    def forward(self, x):\n        x = self.mlp_network(x)\n \n        out_1 = self.last_layer_mean_bmi(x)\n        out_2 = self.last_layer_median_bmi(x)\n        out_3 = self.last_layer_Unmet_Need_Rate(x)\n        out_4 = self.last_layer_Under5_Mortality_Rate(x)\n        out_5 = self.last_layer_Skilled_Birth_Attendant_Rate(x)\n        out_6 = self.last_layer_Stunted_Rate(x)\n\n        return out_1, out_2, out_3, out_4, out_5, out_6 ","metadata":{"execution":{"iopub.status.busy":"2023-07-29T11:38:20.810061Z","iopub.execute_input":"2023-07-29T11:38:20.810493Z","iopub.status.idle":"2023-07-29T11:38:20.823693Z","shell.execute_reply.started":"2023-07-29T11:38:20.810462Z","shell.execute_reply":"2023-07-29T11:38:20.822523Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"y_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:59.797985Z","iopub.execute_input":"2023-07-29T10:04:59.798353Z","iopub.status.idle":"2023-07-29T10:04:59.819327Z","shell.execute_reply.started":"2023-07-29T10:04:59.798320Z","shell.execute_reply":"2023-07-29T10:04:59.818396Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Index(['Mean_BMI', 'Median_BMI', 'Unmet_Need_Rate', 'Under5_Mortality_Rate',\n       'Skilled_Birth_Attendant_Rate', 'Stunted_Rate'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"**Collate function** : process data samples before they are passed to the data loader","metadata":{}},{"cell_type":"code","source":"def collate_fn(data):\n    x, y = data\n    x_inp = torch.tensor(x.values, dtype=torch.float32, device=device)\n    y_out1 = torch.tensor(y[\"Mean_BMI\"].values, dtype=torch.float32, device=device)\n    y_out2 = torch.tensor(y[\"Median_BMI\"].values, dtype=torch.float32, device=device)\n    y_out3 = torch.tensor(y[\"Unmet_Need_Rate\"].values, dtype=torch.float32, device=device)\n    y_out4 = torch.tensor(y[\"Under5_Mortality_Rate\"].values, dtype=torch.float32, device=device)\n    y_out5 = torch.tensor(y[\"Skilled_Birth_Attendant_Rate\"].values, dtype=torch.float32, device=device)\n    y_out6 = torch.tensor(y[\"Stunted_Rate\"].values, dtype=torch.float32, device=device)\n    return x_inp, y_out1, y_out2, y_out3, y_out4, y_out5, y_out6","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:59.821703Z","iopub.execute_input":"2023-07-29T10:04:59.823105Z","iopub.status.idle":"2023-07-29T10:04:59.836216Z","shell.execute_reply.started":"2023-07-29T10:04:59.823014Z","shell.execute_reply":"2023-07-29T10:04:59.834918Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Dataloaders","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    TensorDataset(*collate_fn((x_train1, y_train1))), batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:04:59.837690Z","iopub.execute_input":"2023-07-29T10:04:59.838745Z","iopub.status.idle":"2023-07-29T10:05:00.064882Z","shell.execute_reply.started":"2023-07-29T10:04:59.838707Z","shell.execute_reply":"2023-07-29T10:05:00.063600Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test_dataloader = DataLoader(\n    TensorDataset(*collate_fn((x_test1, y_test1))), batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:05:00.066514Z","iopub.execute_input":"2023-07-29T10:05:00.066913Z","iopub.status.idle":"2023-07-29T10:05:00.093462Z","shell.execute_reply.started":"2023-07-29T10:05:00.066876Z","shell.execute_reply":"2023-07-29T10:05:00.091944Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"val_dataloader = DataLoader(\n    TensorDataset(*collate_fn((x_val1, y_val1))), batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T10:05:00.095541Z","iopub.execute_input":"2023-07-29T10:05:00.096023Z","iopub.status.idle":"2023-07-29T10:05:00.120467Z","shell.execute_reply.started":"2023-07-29T10:05:00.095980Z","shell.execute_reply":"2023-07-29T10:05:00.119559Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Model calling**","metadata":{}},{"cell_type":"code","source":"model = MultiTaskLearn(embed_dim=2000, layers=[2000, 1000, 500, 100]).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:30:48.397983Z","iopub.execute_input":"2023-07-29T12:30:48.398362Z","iopub.status.idle":"2023-07-29T12:30:48.429568Z","shell.execute_reply.started":"2023-07-29T12:30:48.398331Z","shell.execute_reply":"2023-07-29T12:30:48.428403Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"Model evaluation function for selecting the best layers and sizes","metadata":{}},{"cell_type":"code","source":"def model_evaluation(model, dataloader):\n    mse_loss = []\n    \n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            x, y1, y2, y3, y4, y5, y6 = batch\n            out_1, out_2, out_3, out_4, out_5, out_6 = model(x)\n            out_1, out_2, out_3, out_4, out_5, out_6 = out_1.squeeze(), out_2.squeeze(), out_3.squeeze(), out_4.squeeze(), out_5.squeeze(), out_6.squeeze()\n            loss_1 = criterion(out_1, y1)\n            loss_2 = criterion(out_2, y2)\n            loss_3 = criterion(out_3, y3)\n            loss_4 = criterion(out_4, y4)\n            loss_5 = criterion(out_5, y5)\n            loss_6 = criterion(out_6, y6)\n            loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5 + loss_6\n            mse_loss.append(loss.item())\n\n    mse_loss_avg = np.array(mse_loss).mean()\n    return mse_loss_avg","metadata":{"execution":{"iopub.status.busy":"2023-07-29T11:51:27.930575Z","iopub.execute_input":"2023-07-29T11:51:27.931167Z","iopub.status.idle":"2023-07-29T11:51:27.941433Z","shell.execute_reply.started":"2023-07-29T11:51:27.931130Z","shell.execute_reply":"2023-07-29T11:51:27.940333Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"Selection of **best number of layers and sizes of layers**","metadata":{}},{"cell_type":"code","source":"best_model = None\nbest_loss = float('inf')\nbest_layers = None\n\n# Define a set of hyperparameters to search over\nlayer_sizes_list = [[1000, 500, 100], [1500, 750, 250], [2000, 1000, 500]]\nnum_layers_list = [3, 4, 5]\n\nfor layer_sizes in layer_sizes_list:\n    for num_layers in num_layers_list:\n        # Create and train the model with the current hyperparameters\n        model = MultiTaskLearn(embed_dim=2000, layers=[2000] + layer_sizes * (num_layers - 1)).to(device)\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n        for epoch in range(epochs):\n            print(\"Training of the Multi Task Learning model \")\n            training_loss = []\n            mse_1 = []\n            mse_2 = []\n            mse_3 = []\n            mse_4 = []\n            mse_5 = []\n            mse_6 = []\n\n            i = 0\n            for batch in train_dataloader:\n                x, y1, y2, y3, y4, y5, y6 = batch\n                out_1, out_2, out_3, out_4, out_5, out_6 = model(x)\n                out_1, out_2, out_3, out_4, out_5, out_6 = out_1.squeeze(), out_2.squeeze(), out_3.squeeze(),out_4.squeeze(),out_5.squeeze(),out_6.squeeze()\n\n                loss_1 = criterion(out_1, y1)\n                loss_2 = criterion(out_2, y2)\n                loss_3 = criterion(out_3, y3)\n                loss_4 = criterion(out_4, y4)\n                loss_5 = criterion(out_5, y5)\n                loss_6 = criterion(out_6, y6)\n                loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5 + loss_6\n\n                training_loss.append(loss.item())\n                mse_1.append(loss_1.item())\n                mse_2.append(loss_2.item())\n                mse_3.append(loss_3.item())\n                mse_4.append(loss_4.item())\n                mse_5.append(loss_5.item())\n                mse_6.append(loss_6.item())\n\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                i += 1\n    \n            training_loss_avg = np.array(training_loss).mean()\n            train_mse_1 = np.array(mse_1).mean()\n            train_mse_2 = np.array(mse_2).mean()\n            train_mse_3 = np.array(mse_3).mean()\n            train_mse_4 = np.array(mse_4).mean()\n            train_mse_5 = np.array(mse_5).mean()\n            train_mse_6 = np.array(mse_6).mean()\n            \n            print('For validation data...')\n            model_evaluation(model,val_dataloader)\n            \n\n        # Evaluate the model on the validation set\n        val_loss = model_evaluation(model, val_dataloader)\n\n        # Check if this is the best model so far\n        if val_loss < best_loss:\n            best_model = model\n            best_loss = val_loss\n            best_layers = (num_layers, layer_sizes)\n\nprint(\"Best number of layers and layer sizes:\", best_layers)\nprint(\"Best validation loss:\", best_loss)\n\n# Finally, evaluate the best model on the test set\nprint(\"Final evaluation on the test set:\")\nmodel_evaluation(best_model, test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T11:51:30.465631Z","iopub.execute_input":"2023-07-29T11:51:30.466041Z","iopub.status.idle":"2023-07-29T12:25:24.193866Z","shell.execute_reply.started":"2023-07-29T11:51:30.466006Z","shell.execute_reply":"2023-07-29T12:25:24.192903Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Training of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nTraining of the Multi Task Learning model \nFor validation data...\nBest number of layers and layer sizes: (3, [1000, 500, 100])\nBest validation loss: 1046.6879361262088\nFinal evaluation on the test set:\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"1073.2310667194304"},"metadata":{}}]},{"cell_type":"markdown","source":"So from this, we are selecting the number of layers as **3** and the size of each layers is **1000,500,100** respectively","metadata":{}},{"cell_type":"markdown","source":"Model evaluation function for showing the **squared pearson r** value of the test and validation data","metadata":{}},{"cell_type":"code","source":"def model_evaluation(model, dataloader):\n    mse_1 = []\n    mse_2 = []\n    mse_3 = []\n    mse_4 = []\n    mse_5 = []\n    mse_6 = []\n\n    preds_1 = []\n    preds_2 = []\n    preds_3 = []\n    preds_4 = []\n    preds_5 = []\n    preds_6 = []\n\n    y_1 = []\n    y_2 = []\n    y_3 = []\n    y_4 = []\n    y_5 = []\n    y_6 = []\n\n    mse_loss = []\n\n    for i, batch in enumerate(dataloader):\n        x, y1, y2, y3, y4, y5, y6 = batch\n        with torch.no_grad():\n            out_1, out_2, out_3, out_4, out_5, out_6 = model(x)\n            out_1, out_2, out_3, out_4, out_5, out_6 = out_1.squeeze(), out_2.squeeze(), out_3.squeeze(), out_4.squeeze(), out_5.squeeze(), out_6.squeeze()\n            loss_1 = criterion(out_1, y1)\n            loss_2 = criterion(out_2, y2)\n            loss_3 = criterion(out_3, y3)\n            loss_4 = criterion(out_4, y4)\n            loss_5 = criterion(out_5, y5)\n            loss_6 = criterion(out_6, y6)\n            loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5 + loss_6\n\n            mse_1.append(loss_1.item())\n            mse_2.append(loss_2.item())\n            mse_3.append(loss_3.item())\n            mse_4.append(loss_4.item())\n            mse_5.append(loss_5.item())\n            mse_6.append(loss_6.item())\n            mse_loss.append(loss.item())\n\n            y_1.append(y1.cpu().numpy())\n            y_2.append(y2.cpu().numpy())\n            y_3.append(y3.cpu().numpy())\n            y_4.append(y4.cpu().numpy())\n            y_5.append(y5.cpu().numpy())\n            y_6.append(y6.cpu().numpy())\n\n            preds_1_numpy = out_1.detach().cpu().numpy()\n            preds_1.append(preds_1_numpy)\n            preds_2_numpy = out_2.detach().cpu().numpy()\n            preds_2.append(preds_2_numpy)\n            preds_3_numpy = out_3.detach().cpu().numpy()\n            preds_3.append(preds_3_numpy)\n            preds_4_numpy = out_4.detach().cpu().numpy()\n            preds_4.append(preds_4_numpy)\n            preds_5_numpy = out_5.detach().cpu().numpy()\n            preds_5.append(preds_5_numpy)\n            preds_6_numpy = out_6.detach().cpu().numpy()\n            preds_6.append(preds_6_numpy)\n\n    # After the for loop, converting lists to tensors\n    y_1 = torch.tensor(np.concatenate(y_1, axis=0))\n    y_2 = torch.tensor(np.concatenate(y_2, axis=0))\n    y_3 = torch.tensor(np.concatenate(y_3, axis=0))\n    y_4 = torch.tensor(np.concatenate(y_4, axis=0))\n    y_5 = torch.tensor(np.concatenate(y_5, axis=0))\n    y_6 = torch.tensor(np.concatenate(y_6, axis=0))\n\n    # Converting predictions lists to arrays\n    preds_1 = np.concatenate(preds_1, axis=0)\n    preds_2 = np.concatenate(preds_2, axis=0)\n    preds_3 = np.concatenate(preds_3, axis=0)\n    preds_4 = np.concatenate(preds_4, axis=0)\n    preds_5 = np.concatenate(preds_5, axis=0)\n    preds_6 = np.concatenate(preds_6, axis=0)\n\n    mse_loss_avg = np.array(mse_loss).mean()\n    mse_1_avg = np.array(mse_1).mean()\n    mse_2_avg = np.array(mse_2).mean()\n    mse_3_avg = np.array(mse_3).mean()\n    mse_4_avg = np.array(mse_4).mean()\n    mse_5_avg = np.array(mse_5).mean()\n    mse_6_avg = np.array(mse_6).mean()\n    print('average loss is {}'.format(mse_loss_avg))\n\n    r2_column1, _ = stats.pearsonr(preds_1, y_1)\n    r2_column1 = r2_column1**2\n    print('r2 of column1 is {}'.format(r2_column1))\n    r2_column2, _ = stats.pearsonr(preds_2, y_2)\n    r2_column2 = r2_column2**2\n    print('r2 of column2 is {}'.format(r2_column2))\n    r2_column3, _ = stats.pearsonr(preds_3, y_3)\n    r2_column3 = r2_column3**2\n    print('r2 of column3 is {}'.format(r2_column3))\n    r2_column4, _ = stats.pearsonr(preds_4, y_4)\n    r2_column4 = r2_column4**2\n    print('r2 of column4 is {}'.format(r2_column4))\n    r2_column5, _ = stats.pearsonr(preds_5, y_5)\n    r2_column5 = r2_column5**2\n    print('r2 of column5 is {}'.format(r2_column5))\n    r2_column6, _ = stats.pearsonr(preds_6, y_6)\n    r2_column6 = r2_column6**2\n    print('r2 of column6 is {}'.format(r2_column6))","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:49:27.916221Z","iopub.execute_input":"2023-07-29T12:49:27.916669Z","iopub.status.idle":"2023-07-29T12:49:27.944999Z","shell.execute_reply.started":"2023-07-29T12:49:27.916632Z","shell.execute_reply":"2023-07-29T12:49:27.943947Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"Training the model","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    print(\"Training of the Multi Task Learning model \")\n    training_loss = []\n    mse_1 = []\n    mse_2 = []\n    mse_3 = []\n    mse_4 = []\n    mse_5 = []\n    mse_6 = []\n\n    i = 0\n    for batch in train_dataloader:\n        x, y1, y2, y3, y4, y5, y6 = batch\n        out_1, out_2, out_3, out_4, out_5, out_6 = model(x)\n        out_1, out_2, out_3, out_4, out_5, out_6 = out_1.squeeze(), out_2.squeeze(), out_3.squeeze(),out_4.squeeze(),out_5.squeeze(),out_6.squeeze()\n\n        loss_1 = criterion(out_1, y1)\n        loss_2 = criterion(out_2, y2)\n        loss_3 = criterion(out_3, y3)\n        loss_4 = criterion(out_4, y4)\n        loss_5 = criterion(out_5, y5)\n        loss_6 = criterion(out_6, y6)\n        loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5 + loss_6\n\n        training_loss.append(loss.item())\n        mse_1.append(loss_1.item())\n        mse_2.append(loss_2.item())\n        mse_3.append(loss_3.item())\n        mse_4.append(loss_4.item())\n        mse_5.append(loss_5.item())\n        mse_6.append(loss_6.item())\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        i += 1\n    \n    training_loss_avg = np.array(training_loss).mean()\n    train_mse_1 = np.array(mse_1).mean()\n    train_mse_2 = np.array(mse_2).mean()\n    train_mse_3 = np.array(mse_3).mean()\n    train_mse_4 = np.array(mse_4).mean()\n    train_mse_5 = np.array(mse_5).mean()\n    train_mse_6 = np.array(mse_6).mean()\n    print('For test data....')\n    model_evaluation(model,test_dataloader)\n    print('For validation data...')\n    model_evaluation(model,val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:31:09.955030Z","iopub.execute_input":"2023-07-29T12:31:09.955430Z","iopub.status.idle":"2023-07-29T12:33:35.365644Z","shell.execute_reply.started":"2023-07-29T12:31:09.955398Z","shell.execute_reply":"2023-07-29T12:33:35.361883Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Training of the Multi Task Learning model \nFor test data....\naverage loss is 1202.1843070358527\nr2 of column1 is 0.30930042622374504\nr2 of column2 is 0.32080338330315705\nr2 of column3 is 0.49522479782894346\nr2 of column4 is 0.2986705413758222\nr2 of column5 is 0.4371420416765243\nr2 of column6 is 0.28268565423212955\nFor validation data...\naverage loss is 1174.1937646709505\nr2 of column1 is 0.3123601339983473\nr2 of column2 is 0.32576116507950015\nr2 of column3 is 0.5109982397545553\nr2 of column4 is 0.3058558219076788\nr2 of column5 is 0.44367896739196205\nr2 of column6 is 0.2991372388478716\nTraining of the Multi Task Learning model \nFor test data....\naverage loss is 1128.0763547303247\nr2 of column1 is 0.3955947440382598\nr2 of column2 is 0.39777350977233583\nr2 of column3 is 0.5194993421856449\nr2 of column4 is 0.334116629022681\nr2 of column5 is 0.4785398033790418\nr2 of column6 is 0.31387460278426427\nFor validation data...\naverage loss is 1102.463246329886\nr2 of column1 is 0.3994911664968064\nr2 of column2 is 0.40230580660777493\nr2 of column3 is 0.5353604587373522\nr2 of column4 is 0.34280190393689397\nr2 of column5 is 0.4831848826975001\nr2 of column6 is 0.32944224959583246\nTraining of the Multi Task Learning model \nFor test data....\naverage loss is 1100.0243103777775\nr2 of column1 is 0.45036454644838086\nr2 of column2 is 0.4520919871017705\nr2 of column3 is 0.5287506628155136\nr2 of column4 is 0.35207974004364817\nr2 of column5 is 0.49307619324001506\nr2 of column6 is 0.3282613175430638\nFor validation data...\naverage loss is 1074.3490932026848\nr2 of column1 is 0.4571032485198504\nr2 of column2 is 0.4572885795050198\nr2 of column3 is 0.5441047135763908\nr2 of column4 is 0.36186295255588746\nr2 of column5 is 0.4979961926212454\nr2 of column6 is 0.3447645090210644\nTraining of the Multi Task Learning model \nFor test data....\naverage loss is 1084.5343439070905\nr2 of column1 is 0.4925799401657576\nr2 of column2 is 0.48955314180958837\nr2 of column3 is 0.5337735033520898\nr2 of column4 is 0.36356286802407445\nr2 of column5 is 0.5010608201773025\nr2 of column6 is 0.33610127153782926\nFor validation data...\naverage loss is 1058.7736040959592\nr2 of column1 is 0.5010282908521853\nr2 of column2 is 0.49625898849010314\nr2 of column3 is 0.5487168148281307\nr2 of column4 is 0.37322674250802346\nr2 of column5 is 0.5061053030623236\nr2 of column6 is 0.35370441115311474\nTraining of the Multi Task Learning model \nFor test data....\naverage loss is 1075.3329123825324\nr2 of column1 is 0.5178357706230132\nr2 of column2 is 0.515307361828273\nr2 of column3 is 0.5367094581459316\nr2 of column4 is 0.37033925765015707\nr2 of column5 is 0.506125939074701\nr2 of column6 is 0.3412906063818673\nFor validation data...\naverage loss is 1049.6320763259637\nr2 of column1 is 0.5265348895216895\nr2 of column2 is 0.5230125236466573\nr2 of column3 is 0.5512195205826181\nr2 of column4 is 0.3793666630456016\nr2 of column5 is 0.5112671147862548\nr2 of column6 is 0.3596333606401764\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now evaluating for **submission data**","metadata":{}},{"cell_type":"code","source":"train_data_full_dataloader = DataLoader(\n    TensorDataset(*collate_fn((x_train_scaled, y_train))), batch_size=batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:33:50.180892Z","iopub.execute_input":"2023-07-29T12:33:50.181272Z","iopub.status.idle":"2023-07-29T12:33:50.472978Z","shell.execute_reply.started":"2023-07-29T12:33:50.181242Z","shell.execute_reply":"2023-07-29T12:33:50.471916Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    print(\"Training of the Multi Task Learning model for prediction\")\n    for batch in train_data_full_dataloader:\n        x, y1, y2, y3, y4, y5, y6 = batch\n        out_1, out_2, out_3, out_4, out_5, out_6 = model(x)\n        out_1, out_2, out_3, out_4, out_5, out_6 = out_1.squeeze(), out_2.squeeze(), out_3.squeeze(),out_4.squeeze(),out_5.squeeze(),out_6.squeeze()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:09:29.999391Z","iopub.execute_input":"2023-07-29T13:09:29.999794Z","iopub.status.idle":"2023-07-29T13:10:21.293650Z","shell.execute_reply.started":"2023-07-29T13:09:29.999762Z","shell.execute_reply":"2023-07-29T13:10:21.292512Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"Training of the Multi Task Learning model for prediction\nTraining of the Multi Task Learning model for prediction\nTraining of the Multi Task Learning model for prediction\nTraining of the Multi Task Learning model for prediction\nTraining of the Multi Task Learning model for prediction\n","output_type":"stream"}]},{"cell_type":"code","source":"x_submission_tensor = torch.tensor(x_submission_scaled.values, dtype=torch.float32, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:37:24.145585Z","iopub.execute_input":"2023-07-29T12:37:24.146873Z","iopub.status.idle":"2023-07-29T12:37:24.205695Z","shell.execute_reply.started":"2023-07-29T12:37:24.146818Z","shell.execute_reply":"2023-07-29T12:37:24.204544Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:37:40.151570Z","iopub.execute_input":"2023-07-29T12:37:40.151993Z","iopub.status.idle":"2023-07-29T12:37:40.160126Z","shell.execute_reply.started":"2023-07-29T12:37:40.151959Z","shell.execute_reply":"2023-07-29T12:37:40.158937Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"MultiTaskLearn(\n  (mlp_network): Sequential(\n    (0): Linear(in_features=2000, out_features=1000, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=1000, out_features=500, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=500, out_features=100, bias=True)\n    (5): ReLU()\n  )\n  (last_layer_mean_bmi): Linear(in_features=100, out_features=1, bias=True)\n  (last_layer_median_bmi): Linear(in_features=100, out_features=1, bias=True)\n  (last_layer_Unmet_Need_Rate): Linear(in_features=100, out_features=1, bias=True)\n  (last_layer_Under5_Mortality_Rate): Linear(in_features=100, out_features=1, bias=True)\n  (last_layer_Skilled_Birth_Attendant_Rate): Linear(in_features=100, out_features=1, bias=True)\n  (last_layer_Stunted_Rate): Linear(in_features=100, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Make predictions\nwith torch.no_grad():\n    out_1, out_2, out_3, out_4, out_5, out_6 = model(x_submission_tensor)\n    out_1, out_2, out_3, out_4, out_5, out_6 = (\n        out_1.squeeze().cpu().numpy(),\n        out_2.squeeze().cpu().numpy(),\n        out_3.squeeze().cpu().numpy(),\n        out_4.squeeze().cpu().numpy(),\n        out_5.squeeze().cpu().numpy(),\n        out_6.squeeze().cpu().numpy(),\n    )","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:10:21.296037Z","iopub.execute_input":"2023-07-29T13:10:21.296911Z","iopub.status.idle":"2023-07-29T13:10:21.881026Z","shell.execute_reply.started":"2023-07-29T13:10:21.296862Z","shell.execute_reply":"2023-07-29T13:10:21.880067Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(\n    {\n        \"Mean_BMI\": out_1,\n        \"Median_BMI\": out_2,\n        \"Unmet_Need_Rate\": out_3,\n        \"Under5_Mortality_Rate\": out_4,\n        \"Skilled_Birth_Attendant_Rate\": out_5,\n        \"Stunted_Rate\": out_6,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:10:21.882237Z","iopub.execute_input":"2023-07-29T13:10:21.882553Z","iopub.status.idle":"2023-07-29T13:10:21.888665Z","shell.execute_reply.started":"2023-07-29T13:10:21.882524Z","shell.execute_reply":"2023-07-29T13:10:21.887890Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:10:21.890724Z","iopub.execute_input":"2023-07-29T13:10:21.891304Z","iopub.status.idle":"2023-07-29T13:10:21.915495Z","shell.execute_reply.started":"2023-07-29T13:10:21.891275Z","shell.execute_reply":"2023-07-29T13:10:21.914653Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"        Mean_BMI  Median_BMI  Unmet_Need_Rate  Under5_Mortality_Rate  \\\n0      26.243416   25.924145        40.778206               3.112854   \n1      26.242855   25.935745        40.969955               3.078196   \n2      24.683704   24.363781        41.692085               3.629073   \n3      25.073139   24.751678        32.125931               1.528121   \n4      24.474855   24.235176        36.990551               2.135711   \n...          ...         ...              ...                    ...   \n14995  23.646233   22.745113        23.654175               7.972811   \n14996  26.154629   25.739574        20.133526               6.626734   \n14997  24.068684   23.344154        17.618486               4.996466   \n14998  25.346584   24.919716        16.007450               5.925478   \n14999  23.989201   23.612614        15.754584               5.586153   \n\n       Skilled_Birth_Attendant_Rate  Stunted_Rate  \n0                         72.814270     12.554750  \n1                         71.280876     12.051136  \n2                         65.287247     13.288667  \n3                         74.187904      9.784106  \n4                         70.717033     11.025850  \n...                             ...           ...  \n14995                     73.180389     29.109512  \n14996                     86.554520     28.000225  \n14997                     84.460472     19.504204  \n14998                     92.712708     24.596518  \n14999                     88.239746     23.594503  \n\n[15000 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean_BMI</th>\n      <th>Median_BMI</th>\n      <th>Unmet_Need_Rate</th>\n      <th>Under5_Mortality_Rate</th>\n      <th>Skilled_Birth_Attendant_Rate</th>\n      <th>Stunted_Rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26.243416</td>\n      <td>25.924145</td>\n      <td>40.778206</td>\n      <td>3.112854</td>\n      <td>72.814270</td>\n      <td>12.554750</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26.242855</td>\n      <td>25.935745</td>\n      <td>40.969955</td>\n      <td>3.078196</td>\n      <td>71.280876</td>\n      <td>12.051136</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24.683704</td>\n      <td>24.363781</td>\n      <td>41.692085</td>\n      <td>3.629073</td>\n      <td>65.287247</td>\n      <td>13.288667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25.073139</td>\n      <td>24.751678</td>\n      <td>32.125931</td>\n      <td>1.528121</td>\n      <td>74.187904</td>\n      <td>9.784106</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24.474855</td>\n      <td>24.235176</td>\n      <td>36.990551</td>\n      <td>2.135711</td>\n      <td>70.717033</td>\n      <td>11.025850</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14995</th>\n      <td>23.646233</td>\n      <td>22.745113</td>\n      <td>23.654175</td>\n      <td>7.972811</td>\n      <td>73.180389</td>\n      <td>29.109512</td>\n    </tr>\n    <tr>\n      <th>14996</th>\n      <td>26.154629</td>\n      <td>25.739574</td>\n      <td>20.133526</td>\n      <td>6.626734</td>\n      <td>86.554520</td>\n      <td>28.000225</td>\n    </tr>\n    <tr>\n      <th>14997</th>\n      <td>24.068684</td>\n      <td>23.344154</td>\n      <td>17.618486</td>\n      <td>4.996466</td>\n      <td>84.460472</td>\n      <td>19.504204</td>\n    </tr>\n    <tr>\n      <th>14998</th>\n      <td>25.346584</td>\n      <td>24.919716</td>\n      <td>16.007450</td>\n      <td>5.925478</td>\n      <td>92.712708</td>\n      <td>24.596518</td>\n    </tr>\n    <tr>\n      <th>14999</th>\n      <td>23.989201</td>\n      <td>23.612614</td>\n      <td>15.754584</td>\n      <td>5.586153</td>\n      <td>88.239746</td>\n      <td>23.594503</td>\n    </tr>\n  </tbody>\n</table>\n<p>15000 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Concatenate the DataFrames horizontally\nsubmission7 = pd.concat([DHSID, submission_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:10:21.917615Z","iopub.execute_input":"2023-07-29T13:10:21.918140Z","iopub.status.idle":"2023-07-29T13:10:21.927531Z","shell.execute_reply.started":"2023-07-29T13:10:21.918110Z","shell.execute_reply":"2023-07-29T13:10:21.926707Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"Submission file","metadata":{}},{"cell_type":"code","source":"submission7.to_csv('submission7.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:10:21.928582Z","iopub.execute_input":"2023-07-29T13:10:21.928924Z","iopub.status.idle":"2023-07-29T13:10:22.097309Z","shell.execute_reply.started":"2023-07-29T13:10:21.928882Z","shell.execute_reply":"2023-07-29T13:10:22.096443Z"},"trusted":true},"execution_count":107,"outputs":[]}]}